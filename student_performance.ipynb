# import the data file
import pandas as pd
df = pd.read_csv('student-mat.csv', sep=';')
display(df.head())

# Examine Data Shape and Info
print("Data Shape:", df.shape)
display(df.info())

# Descriptive Statistics
display(df.describe())

# Target Variable Analysis
print("\nTarget Variable (G3) Statistics:")
print("Mean:", df['G3'].mean())
print("Median:", df['G3'].median())
print("Standard Deviation:", df['G3'].std())
display(df['G3'].describe())

import matplotlib.pyplot as plt
plt.figure(figsize=(8, 6))
plt.hist(df['G3'], bins=10, color='skyblue', edgecolor='black')
plt.xlabel("Final Grade (G3)")
plt.ylabel("Frequency")
plt.title("Distribution of Final Grades")
plt.show()

# Feature Correlation
correlation_matrix = df.corr()
display(correlation_matrix['G3'].sort_values(ascending=False))
plt.figure(figsize=(10, 8))
import seaborn as sns
sns.heatmap(correlation_matrix[['G3']].sort_values(by='G3', ascending=False), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation with G3')
plt.show()


# Categorical Feature Exploration
categorical_features = ['school', 'internet', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob', 'reason', 'guardian', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'romantic']
for col in categorical_features:
    print(f"\nUnique values and counts for {col}:")
    display(df[col].value_counts())
    plt.figure(figsize=(8, 6))
    df[col].value_counts().plot(kind='bar', color='lightcoral')
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.show()

import seaborn as sns
# Feature Correlation (numerical features only)
numerical_features = df.select_dtypes(include=['int64', 'float64']).columns
correlation_matrix = df[numerical_features].corr()
display(correlation_matrix['G3'].sort_values(ascending=False))

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix[['G3']].sort_values(by='G3', ascending=False), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation with G3 (Numerical Features)')
plt.show()

# Relevant Columns Identification (manual selection based on exploration)
relevant_columns = ['G1', 'G2', 'age', 'studytime', 'internet', 'goout', 'school', 'address', 'failures', 'higher', 'absences']
print("\nRelevant Columns for Prediction:")
print(relevant_columns)


# Missing Value Handling Strategy
print("\nMissing Value Handling Strategy:")
print("Based on the .info() method, there are no missing values in the dataset.  However, if missing values were present in future datasets, a strategy would depend on the nature and amount of missing data.  For numerical features, imputation with the mean or median could be considered.  For categorical features, imputation with the mode or a separate 'missing' category might be appropriate.  Advanced techniques like K-Nearest Neighbors imputation could also be used.")

import pandas as pd
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# 1. Handle Missing Values (though none were found in exploration, double-check)
# No action needed if no missing values are present.

# 2. Convert Categorical Features
categorical_cols = ['school', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob', 'reason', 'guardian', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic']
binary_cols = ['internet', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'romantic']

# Label Encoding for binary features
label_encoder = LabelEncoder()
for col in binary_cols:
    df[col] = label_encoder.fit_transform(df[col])

# One-Hot Encoding for other categorical features
df = pd.get_dummies(df, columns=[col for col in categorical_cols if col not in binary_cols], drop_first=True)


# 3. Select Relevant Features
relevant_cols = ['G1', 'G2', 'age', 'studytime', 'internet', 'goout', 'school_MS', 'address_U', 'failures', 'higher', 'absences', 'G3']
df_selected = df[relevant_cols]

# 4. Verify Data Types
print(df_selected.dtypes)

# Check for non-numerical features in selected columns
non_numerical_cols = df_selected.select_dtypes(exclude=['number']).columns
if len(non_numerical_cols) > 0:
    print("Non-numerical columns found:", non_numerical_cols)
    # Handle any non-numerical features found here if necessary.
else:
    print("All selected features are numerical.")

display(df_selected.head())

# Convert boolean columns to integers
df_selected['school_MS'] = df_selected['school_MS'].astype(int)
df_selected['address_U'] = df_selected['address_U'].astype(int)

# Verify data types again
print(df_selected.dtypes)

# Check for non-numerical features in selected columns again
non_numerical_cols = df_selected.select_dtypes(exclude=['number']).columns
if len(non_numerical_cols) > 0:
    print("Non-numerical columns found:", non_numerical_cols)
else:
    print("All selected features are numerical.")
    
display(df_selected.head())

from sklearn.model_selection import train_test_split

# Assuming df_selected is already prepared as per the previous steps
X = df_selected.drop('G3', axis=1)
y = df_selected['G3']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

import pandas as pd
from sklearn.preprocessing import StandardScaler

# 1. Create combined grade feature
X_train['G1_G2_avg'] = (X_train['G1'] + X_train['G2']) / 2
X_test['G1_G2_avg'] = (X_test['G1'] + X_test['G2']) / 2

# 2. Scale numerical features
numerical_cols = ['G1', 'G2', 'age', 'studytime', 'failures', 'absences', 'G1_G2_avg', 'goout']
scaler = StandardScaler()
X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])
X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])

display(X_train.head())
display(X_test.head())

from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier

# Regression Models
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

dt_reg_model = DecisionTreeRegressor()
dt_reg_model.fit(X_train, y_train)

rf_reg_model = RandomForestRegressor()
rf_reg_model.fit(X_train, y_train)

# Classification Models
passing_threshold = 10
y_train_class = (y_train >= passing_threshold).astype(int)
y_test_class = (y_test >= passing_threshold).astype(int)

lr_class_model = LogisticRegression()
lr_class_model.fit(X_train, y_train_class)

dt_class_model = DecisionTreeClassifier()
dt_class_model.fit(X_train, y_train_class)

rf_class_model = RandomForestClassifier()
rf_class_model.fit(X_train, y_train_class)

import numpy as np
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import r2_score, mean_squared_error, accuracy_score, f1_score, precision_score, recall_score

# Define parameter grids for each model
param_grid_lr_reg = {'fit_intercept': [True, False]}
param_grid_dt_reg = {'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10]}
param_grid_rf_reg = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]}
param_grid_lr_class = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2']}
param_grid_dt_class = {'max_depth': [None, 10, 20], 'min_samples_split': [2, 5]}
param_grid_rf_class = {'n_estimators': [50, 100], 'max_depth': [None, 10]}

# Perform hyperparameter tuning using GridSearchCV for each model
models = [
    (lr_model, param_grid_lr_reg, 'r2'),
    (dt_reg_model, param_grid_dt_reg, 'r2'),
    (rf_reg_model, param_grid_rf_reg, 'r2'),
    (lr_class_model, param_grid_lr_class, 'accuracy'),
    (dt_class_model, param_grid_dt_class, 'accuracy'),
    (rf_class_model, param_grid_rf_class, 'accuracy')
]

for model, param_grid, scoring in models:
    grid_search = GridSearchCV(model, param_grid, cv=5, scoring=scoring)
    if scoring == 'r2':
        grid_search.fit(X_train, y_train)
    else:  # classification
        grid_search.fit(X_train, y_train_class)
    
    best_model = grid_search.best_estimator_

    # Evaluate the best model on both training and testing datasets
    if scoring == 'r2':
        y_train_pred = best_model.predict(X_train)
        y_test_pred = best_model.predict(X_test)
        print(f"Best parameters for {model}: {grid_search.best_params_}")
        print(f"Training R^2: {r2_score(y_train, y_train_pred)}")
        print(f"Testing R^2: {r2_score(y_test, y_test_pred)}")
        train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
        test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
        print(f"Training RMSE: {train_rmse}")
        print(f"Testing RMSE: {test_rmse}")
    else:
        y_train_pred = best_model.predict(X_train)
        y_test_pred = best_model.predict(X_test)
        print(f"Best parameters for {model}: {grid_search.best_params_}")
        print(f"Training Accuracy: {accuracy_score(y_train_class, y_train_pred)}")
        print(f"Testing Accuracy: {accuracy_score(y_test_class, y_test_pred)}")
        print(f"Training F1 Score: {f1_score(y_train_class, y_train_pred)}")
        print(f"Testing F1 Score: {f1_score(y_test_class, y_test_pred)}")
        print(f"Training Precision: {precision_score(y_train_class, y_train_pred)}")
        print(f"Testing Precision: {precision_score(y_test_class, y_test_pred)}")
        print(f"Training Recall: {recall_score(y_train_class, y_train_pred)}")
        print(f"Testing Recall: {recall_score(y_test_class, y_test_pred)}")

# Assign the best models from the previous step to the variables
# Replace with the actual names of your best models if different
best_rf_reg_model = rf_reg_model # Placeholder, replace with the actual best model
best_rf_class_model = rf_class_model # Placeholder, replace with the actual best model

from sklearn.metrics import r2_score, mean_squared_error, accuracy_score, f1_score, precision_score, recall_score
import numpy as np

# Assuming 'best_rf_reg_model' and 'best_rf_class_model' are available from previous steps.
# If they are not available, replace them with the actual names of the optimized models.
try:
    y_pred_rf_reg = best_rf_reg_model.predict(X_test)
    r2_rf_reg = r2_score(y_test, y_pred_rf_reg)
    rmse_rf_reg = np.sqrt(mean_squared_error(y_test, y_pred_rf_reg))
    print(f"Random Forest Regressor - R^2: {r2_rf_reg:.3f}, RMSE: {rmse_rf_reg:.3f}")
    
    print("\nRandom Forest Regressor Feature Importance:")
    for i, col in enumerate(X_train.columns):
        print(f"{col}: {best_rf_reg_model.feature_importances_[i]:.3f}")
    
    
    y_pred_rf_class = best_rf_class_model.predict(X_test)
    accuracy_rf_class = accuracy_score(y_test_class, y_pred_rf_class)
    f1_rf_class = f1_score(y_test_class, y_pred_rf_class)
    precision_rf_class = precision_score(y_test_class, y_pred_rf_class)
    recall_rf_class = recall_score(y_test_class, y_pred_rf_class)

    print(f"\nRandom Forest Classifier - Accuracy: {accuracy_rf_class:.3f}, F1-score: {f1_rf_class:.3f}, Precision: {precision_rf_class:.3f}, Recall: {recall_rf_class:.3f}")

    print("\nRandom Forest Classifier Feature Importance:")
    for i, col in enumerate(X_train.columns):
        print(f"{col}: {best_rf_class_model.feature_importances_[i]:.3f}")

    print("\nDiscussion of Feature Importances:")
    print("This section requires a more in-depth analysis of the feature importance scores to draw meaningful conclusions.")

except NameError:
    print("Error: best_rf_reg_model or best_rf_class_model not found.")

import matplotlib.pyplot as plt
import seaborn as sns

# 1. Model Performance Visualization
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred_rf_reg, alpha=0.5)
plt.xlabel("Actual G3")
plt.ylabel("Predicted G3")
plt.title("Actual vs. Predicted G3 (Random Forest Regressor)")
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')
plt.savefig('actual_vs_predicted_g3.png')
plt.show()


plt.figure(figsize=(8, 6))
residuals = y_test - y_pred_rf_reg
plt.scatter(y_pred_rf_reg, residuals, alpha=0.5)
plt.xlabel("Predicted G3")
plt.ylabel("Residuals")
plt.title("Residual Plot (Random Forest Regressor)")
plt.axhline(y=0, color='red', linestyle='--')
plt.savefig('residual_plot.png')
plt.show()


# 2. Feature Importance Visualization
plt.figure(figsize=(10, 6))
feature_importances = best_rf_reg_model.feature_importances_
feature_names = X_train.columns
sns.barplot(x=feature_importances, y=feature_names)
plt.xlabel('Feature Importance')
plt.ylabel('Features')
plt.title('Feature Importance (Random Forest Regressor)')
plt.savefig('feature_importance_regressor.png')
plt.show()

plt.figure(figsize=(10, 6))
feature_importances = best_rf_class_model.feature_importances_
feature_names = X_train.columns
sns.barplot(x=feature_importances, y=feature_names)
plt.xlabel('Feature Importance')
plt.ylabel('Features')
plt.title('Feature Importance (Random Forest Classifier)')
plt.savefig('feature_importance_classifier.png')
plt.show()


# 3. Relationship Visualization
top_3_features = ['G2', 'G1', 'absences']  # Example, replace with actual top 3 from previous steps
for feature in top_3_features:
    plt.figure(figsize=(6, 4))
    sns.boxplot(x=df_selected[feature], y=df_selected['G3'])
    plt.xlabel(feature)
    plt.ylabel('G3')
    plt.title(f'Relationship between {feature} and G3')
    plt.savefig(f'relationship_{feature}_g3.png')
    plt.show()
    
# 4. Distribution of G3 and Pass/Fail Classification Visualization
plt.figure(figsize=(8, 6))
sns.histplot(df_selected['G3'], kde=True)
plt.axvline(x=10, color='red', linestyle='--', label='Pass/Fail Threshold')
plt.xlabel('Final Grade (G3)')
plt.ylabel('Frequency')
plt.title('Distribution of Final Grades')
plt.legend()
plt.savefig('g3_distribution.png')
plt.show()


plt.figure(figsize=(6, 4))
pass_fail_counts = df_selected['G3'].apply(lambda x: 1 if x >= 10 else 0).value_counts()
plt.bar(pass_fail_counts.index, pass_fail_counts.values, tick_label=['Fail', 'Pass'], color=['red', 'green'])
plt.xlabel('Pass/Fail Status')
plt.ylabel('Count')
plt.title('Pass/Fail Classification Counts')
plt.savefig('pass_fail_counts.png')
plt.show()
